
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import GridSearchCV
import numpy as np


#### ADD DATASET NORMALIZATION

def select_best_datasets_LR_model(reg_potential_data, labels, epsilon = 1e-7, num_datasets = 10, max_iters = 100, penalty_min = -1, penalty_max = 10):
    """
    Select the best n datasets discriminating between query and background genes. Use binary search to tune the regularization parameter of the L1 model. Increasing
    Regularization decreases the number of datasets used.

    epsilon: if a dataset's coefficient exceeds this value, count as selected dataset
    num_datasets: desired number of datasets selected
    max_iters: maximum iterations of binary search
    penalty_min: penalty of LR model defined as C = 2**-penalty, with decreasing C meaning increasing regularization. For penalty_min = -1, C = 2^(-1 * 1) = 2, so less regularized.
    penalty_max: penalty of LR model, for penalty_max = 10, C = 2^-10, a very regularized model

    returns:
    best_datasets: boolean index of datasets selected
    """
    LR_model_kwargs = dict(
            penalty='l1', tol=0.01, dual=False, random_state=999
        )

    def binary_search(low, high, iters = 0, num_datasets_desired = 10, max_iters = 50):
        #instantiate model with penalty as middle of current window
        penalty = (high - low) / 2
        latest_model = LogisticRegression(C=2**-penalty, **LR_model_kwargs).fit(reg_potential_data, labels)
        coefs = latest_model.coef_
        #get upweighted datasets
        best_datasets = coefs > epsilon
        #how many datasets were used?
        num_datasets_selected = best_datasets.sum()
        #break if the desired amount of datasets were used or max iters reached
        if num_datasets_selected == num_datasets_desired or iters == max_iters: 
            return best_datasets, latest_model
        #increment number of iters
        search_kwargs = dict(
            num_datasets_desired = num_datasets_desired, iters = iters += 1, max_iters = max_iters
        )
        #if too many datasets used, shift window to increase penalty
        if num_datasets_selected > num_datasets:
            #start new search with moved window
            return binary_search(penalty, high, **search_kwargs)
        else:
            #decrease penalty if too few datasets used
            return binary_search(low, penalty, **search_kwargs)

    return binary_search(penalty_min, penalty_max, max_iters = max_iters, num_datasets_desired = num_datasetsÃŸ)


def select_best_datasets_ANOVA(reg_potential_data, labels, k_features = 200):
    """
    Use ANOVA feature selector to narrow the field before more expensive LR feature selection
    """
    assert( reg_potential_data.shape[1] <= k_features )

    selected = SelectKBest(f_classif, k=k_features).fit(reg_potential_data, labels).get_support()

    return selected

def build_chromatin_model(reg_potential_data, dataset_metadata, labels, chromatin_model, covariates = None, 
    num_anova_features = 200, epsilon = 1e-7, num_lr_features = 10, max_iters = 20, penalty_min = -1, penalty_max = 10):
    """
    reg_potential_data: numpy matrix of gene x sample of RP data
    labels: generated by background_genes_selection.py, lables for each gene in query vs. background 
    chromatin_model: GridSearchCV sklearn model that trains a model to fit the chromatin landscape. Changes in the activation of this model show strength of effect of ISD
    num_anova_features: number of features to select using ANOVA
    num_LR_features: number of features to select using the Logistic Regression L1 model
    """
    assert( isinstance(reg_potential_data, np.array ) ), 'Regulatory potential data must be provided as numpy array'
    assert( isinstance(labels, np.array) ), 'Labels provided must be of type numpy array'
    assert( isinstance(dataset_metadata, list, np.array))
    assert( dataset_metadata.shape[0] == reg_potential_data.shape[0] and len(dataset_metadata.shape) == 1)

    dataset_metadata = np.array(dataset_metadata)

    #select features with anova
    anova_featues = select_best_datasets_ANOVA(reg_potential_data, labels, k_features = num_anova_features)

    #leave out worst features
    reg_potential_data = reg_potential_data[:, anova_featues]
    dataset_metadata = dataset_metadata[anova_featues]

    #select best features using LR, record some info about model
    lr_features, selection_model = select_best_datasets_LR_model(reg_potential_data, labels, epsilon=epsilon, num_datasets=num_lr_features, 
        max_iters=max_iters, penalty_min=penalty_min, penalty_max=penalty_max)

    #subset for best features
    reg_potential_data = reg_potential_data[:, lr_features] 
    dataset_metadata = dataset_metadata[lr_features]

    #add covariates back to data
    if not covariates is None:
        reg_potential_data = np.concatenate([reg_potential_data, covariates], axis = 1)

    #train chromatin landscape model
    chromatin_model.fit(reg_potential_data, labels, chromatin_model)

    #return final trained model, the selected features, and some model metadata
    return reg_potential_data, dataset_metadata, selection_model


class ChromatinModel():
    def __init__(self):
        raise NotImplementedError("Must implement \"__init__\" method of ChromatinModel")
    def fit(self):
        raise NotImplementedError("Must implement \"fit\" method of ChromatinModel")
    def get_deltaRP_activation(self):
        raise NotImplementedError("Must implement \"get_deltaRP_activation\" method of ChromatinModel")


class LogisticRegressionChromatinModel(ChromatinModel):

    def __init__(self, param_grid, kfold_cv = 4, scoring = 'auc', **regression_kwargs):
        classifier = LogisticRegression(**regression_kwargs)
        self.model = GridSearchCV(classifier, param_grid = param_grid, scoring = scoring, cv = kfold_cv)

    def fit(self, X, y):
        self.model.fit(X, y)
    
    def get_deltaRP_activation(self, X):
        return np.dot(X, self.model.best_estimator_.coef_.reshape((-1, 1)))