
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import GridSearchCV
import numpy as np

def midpoint(low, high): return (high - low) / 2

def select_best_datasets_LR_model(reg_potential_data, labels, epsilon = 1e-7, num_datasets = 10, max_iters = 100, penalty_min = -1, penalty_max = 10):
    """
    Select the best n datasets discriminating between query and background genes. Use binary search to tune the regularization parameter of the L1 model. Increasing
    Regularization decreases the number of datasets used.

    epsilon: if a dataset's coefficient exceeds this value, count as selected dataset
    num_datasets: desired number of datasets selected
    max_iters: maximum iterations of binary search
    penalty_min: penalty of LR model defined as C = 2**-penalty, with decreasing C meaning increasing regularization. For penalty_min = -1, C = 2^(-1 * 1) = 2, so less regularized.
    penalty_max: penalty of LR model, for penalty_max = 10, C = 2^-10, a very regularized model
    """
    LR_model_kwargs = dict(
            penalty='l1', tol=0.01, dual=False, random_state=999
        )

    penalty = midpoint(penalty_min, penalty_max)

    for _ in range(max_iters):
        #define a model using the fixed parameters chosen in LR_model_kwargs
        latest_model = LogisticRegression(C=2**-penalty, **LR_model_kwargs).fit(reg_potential_data, labels)
        coefs = latest_model.coef_
        #get upweighted datasets
        best_datasets = coefs > epsilon
        #how many datasets were used?
        num_datasets_selected = best_datasets.sum()
        #break if the desired amount of datasets were used
        if num_datasets_selected == num_datasets:
            break
        else:
            #update binary search parameters
            if num_datasets_selected > num_datasets:
                penalty_max = midpoint
            else:
                penalty_min = midpoint
            penalty = midpoint(penalty_min, penalty_max)

    return best_datasets, latest_model


def select_best_datasets_ANOVA(reg_potential_data, labels, k_features = 200):
    """
    Use ANOVA feature selector to narrow the field before more expensive LR feature selection
    """
    assert( reg_potential_data.shape[1] <= k_features )

    selected = SelectKBest(f_classif, k=k_features).fit(reg_potential_data, labels).get_support()

    return selected


def fit_chromatin_model(reg_potential_data, labels, model):

    assert( isinstance(model, GridSearchCV) ), "Model provided must be of GridSearchCV class"
    model.fit(reg_potential_data, labels)
    #return best estimator fitted, along with its metric score and model hyperparameters
    return model.best_estimator_, model.best_score_

def build_chromatin_model(reg_potential_data, labels, chromatin_model, covariates = None, 
    num_anova_features = 200, epsilon = 1e-7, num_lr_features = 10, max_iters = 20, penalty_min = -1, penalty_max = 10):
    """
    reg_potential_data: numpy matrix of gene x sample of RP data
    labels: generated by background_genes_selection.py, lables for each gene in query vs. background 
    chromatin_model: GridSearchCV sklearn model that trains a model to fit the chromatin landscape. Changes in the activation of this model show strength of effect of ISD
    num_anova_features: number of features to select using ANOVA
    num_LR_features: number of features to select using the Logistic Regression L1 model
    """
    assert( isinstance(reg_potential_data, np.array ) ), 'Regulatory potential data must be provided as numpy array'
    assert( isinstance(labels, np.array) ), 'Labels provided must be of type numpy array'

    #select features with anova
    anova_featues = select_best_datasets_ANOVA(reg_potential_data, labels, k_features = num_anova_features)

    #leave out worst features
    reg_potential_data = reg_potential_data[:, anova_featues]

    #select best features using LR, record some info about model
    lr_features, selection_model = select_best_datasets_LR_model(reg_potential_data, labels, epsilon=epsilon, num_datasets=num_lr_features, 
        max_iters=max_iters, penalty_min=penalty_min, penalty_max=penalty_max)

    #subset for best features
    reg_potential_data = reg_potential_data[:, lr_features] 

    #add covariates back to data
    if not covariates is None:
        reg_potential_data = np.concatenate([reg_potential_data, covariates], axis = 1)

    #train chromatin landscape model
    chromatin_model, best_chromatin_score = fit_chromatin_model(reg_potential_data, labels, chromatin_model)

    #return final trained model, the selected features, and some model metadata
    return chromatin_model, reg_potential_data, selection_model.coef_, {
        'selection_model_params' : selection_model.get_params(),
        'chromatin_model_params' : chromatin_model.get_params()
        'chromatin_model_score' : best_chromatin_score,
    }