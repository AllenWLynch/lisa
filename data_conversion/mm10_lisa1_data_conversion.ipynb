{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import lisa\n",
    "lisa = importlib.reload(lisa)\n",
    "\n",
    "from lisa.core.data_interface import DataInterface,PACKAGE_PATH\n",
    "from lisa.core.genome_tools import Genome\n",
    "from lisa.core.utils import indices_list_to_sparse_array\n",
    "from lisa.core import data_interface\n",
    "data_interface = importlib.reload(data_interface)\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = 'hg38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_conversion/lisa1_data.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data_path = os.path.join('data_conversion','old_data',species) \n",
    "\n",
    "v1_config = configparser.ConfigParser()\n",
    "v1_config.read(os.path.join('data_conversion', 'lisa1_data.ini'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "\n",
    "Go to NCBI Table browser and select the UCSC refGene table for mm10, output all fields. Download this table as \"mm10.refseq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_conversion/old_data/hg38/hg38_promoter_TADann_H3K4me3_enhance_k27me3_Using.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8f23088a1965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tad_info = pd.read_csv(v1_config.get(species, 'tad_info').format(prefix = old_data_path),\n\u001b[0m\u001b[1;32m      2\u001b[0m                         sep = '\\t')\n\u001b[1;32m      3\u001b[0m \u001b[0mtad_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tad_group'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtad_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk4me3_order_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtad_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtad_order_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/.venvs/lisa_2.1_testing/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/.venvs/lisa_2.1_testing/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/.venvs/lisa_2.1_testing/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/.venvs/lisa_2.1_testing/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/.venvs/lisa_2.1_testing/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_conversion/old_data/hg38/hg38_promoter_TADann_H3K4me3_enhance_k27me3_Using.xls'"
     ]
    }
   ],
   "source": [
    "tad_info = pd.read_csv(v1_config.get(species, 'tad_info').format(prefix = old_data_path),\n",
    "                        sep = '\\t')\n",
    "tad_info['tad_group'] = tad_info.k4me3_order_cluster.astype(str) + '-' + tad_info.tad_order_cluster.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = pd.read_csv(os.path.join(PACKAGE_PATH, 'genomes',species + '.genome'), sep = '\\t', header = None)\n",
    "genome.columns = ['chrom','length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(os.path.join('data_conversion', species + '.refseq'), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = genes.merge(tad_info[['geneName','tad_group']], left_on='name2',\n",
    "            right_on='geneName', how = 'left').fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes.merge(genome['chrom'], on = 'chrom', how = 'right')\\\n",
    "    [['name','chrom','strand','txStart','txEnd','exonStarts','exonEnds','name2', 'tad_group']]\\\n",
    "    .to_csv(os.path.join(PACKAGE_PATH,'genomes',species + '.refseq'), header = None, index = None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "refseq=os.path.join(PACKAGE_PATH,'genomes',species + '.refseq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NM_001039510\tchr1\t-\t134199214\t134235457\t134199214,134234014,134235227,\t134203590,134234412,134235457,\tAdora1\t2-8\r\n",
      "NM_001282945\tchr1\t-\t134199214\t134235457\t134199214,134234014,134235227,\t134203590,134234446,134235457,\tAdora1\t2-8\r\n"
     ]
    }
   ],
   "source": [
    "!head -n2 $refseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "\n",
    "Download old LISA data:\n",
    "\n",
    "```\n",
    "wget -c http://lisa.cistrome.org/cistromedb_data/lisa_v1.2_mm10.tar.gz\n",
    "```\n",
    "and unpack into ./old_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "\n",
    "Instantiate a data inferface using the new genome and genes, then use it to generate RP maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_interface.DataInterface('mm10', window_size=1000, make_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rp_map, enhanced_rp_map = data.build_binned_rp_map('basic',10000), data.build_binned_rp_map('enhanced', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_rp_map('basic_10K', basic_rp_map)\n",
    "data.add_rp_map('enhanced_10K', enhanced_rp_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: \n",
    "\n",
    "Load the old genome format and map to the new bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_genome = Genome.from_file(v1_config.get(species,'chrom_len').format(prefix = old_data_path),\n",
    "                             window_size=1000, _sort=False)\n",
    "\n",
    "genome_map = old_genome.map_genomes(data.genome)\n",
    "genome_map = np.array(genome_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in old metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(v1_config.get('basics','meta').format(prefix = old_data_path), encoding='latin', sep = '\\t')\\\n",
    "    .set_index('id')\n",
    "metadata.index = metadata.index.astype(str)\n",
    "motif_meta = pd.read_csv(v1_config.get('basics','motif').format(prefix = old_data_path), encoding='latin', sep = '\\t')\\\n",
    "    .set_index('id')\n",
    "motif_meta['factor'] = motif_meta.symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "\n",
    "Transfer accessibility arrays to new format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accessibility_data(technology, old_data_path):\n",
    "    \n",
    "    with h5.File(v1_config.get(species, technology + '_count').format(prefix = old_data_path), 'r') as old_data:\n",
    "        dataset_ids = np.array(old_data['IDs'][...]).astype(str)\n",
    "        acc_matrix = np.array(old_data['OrderCount'][...])\n",
    "        \n",
    "    return acc_matrix, dataset_ids\n",
    "\n",
    "def add_accessibility_matrix(data, technology, acc_matrix, dataset_ids,*, rp_maps, rp_map_styles, metadata,\n",
    "                            reads_range):\n",
    "    \n",
    "    headers = data.get_metadata_headers(technology)\n",
    "    \n",
    "    for i, dataset_id in enumerate(dataset_ids):\n",
    "\n",
    "        dataset_id = str(dataset_id).split('_')[0]\n",
    "        sample_metadata = metadata.loc[dataset_id, headers + ['qc']].to_dict()\n",
    "\n",
    "        sample_acc = np.array(acc_matrix[:, i]).reshape(-1)\n",
    "\n",
    "        if int(sample_metadata['qc']) == 1 and (reads_range[0] <= sample_acc.sum() <= reads_range[1]):\n",
    "            data.add_profile_data(technology, str(dataset_id), sample_acc, rp_maps, \n",
    "                                      rp_map_styles, **sample_metadata)\n",
    "            \n",
    "def convert_accessibility(*,data,technology, old_data_path, bin_map, \n",
    "                          rp_maps, rp_map_styles, metadata, reads_range):\n",
    "    \n",
    "    acc_matrix, dataset_ids = load_accessibility_data(technology, old_data_path)\n",
    "\n",
    "    projected_acc_matrix = np.vstack([\n",
    "        data.project_array(r, bin_map, len(data.genome))\n",
    "        for r in acc_matrix.T\n",
    "    ]).T\n",
    "    \n",
    "    \n",
    "    add_accessibility_matrix(data, technology, projected_acc_matrix, dataset_ids, \n",
    "                         rp_maps=rp_maps, rp_map_styles=rp_map_styles,\n",
    "                        metadata = metadata, reads_range = reads_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessibility_kwargs = dict(\n",
    "    data = data,\n",
    "    old_data_path=old_data_path,\n",
    "    bin_map=genome_map, rp_maps = [basic_rp_map, enhanced_rp_map], rp_map_styles = ['basic_10K','enhanced_10K'],\n",
    "    metadata = metadata\n",
    ")\n",
    "\n",
    "convert_accessibility(technology='DNase', **accessibility_kwargs, reads_range = (75000, 160000))\n",
    "\n",
    "convert_accessibility(technology='H3K27ac', **accessibility_kwargs, reads_range = (130000, 160000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:\n",
    "\n",
    "Transfer factor binding to new genome scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_genome_100bp = Genome.from_file(v1_config.get(species,'chrom_len').format(prefix = old_data_path),\n",
    "                             window_size=100, _sort=False)\n",
    "\n",
    "hits_binmap = np.array(old_genome_100bp.map_genomes(data.genome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_matrix(config_key, old_data_path, num_bins, metadata, offset = 0, skip_qc = False):\n",
    "\n",
    "    with h5.File(v1_config.get(species, config_key)\\\n",
    "                 .format(prefix = old_data_path), 'r') as tf_hits:\n",
    "\n",
    "        num_samples = len(list(tf_hits.keys()))\n",
    "        \n",
    "        indices, dataset_ids = [],[]\n",
    "        \n",
    "        for sample in tf_hits.keys():\n",
    "            \n",
    "            if not sample == 'IDs' and not sample == 'TFs':\n",
    "                \n",
    "                try:\n",
    "                    sample_metadata = metadata.loc[sample]\n",
    "                \n",
    "                    if skip_qc or sample_metadata.qc == 1:\n",
    "                        \n",
    "                        factor_name = sample_metadata.factor.replace('/', '-').replace('_','-')\n",
    "\n",
    "                        peaks = tf_hits[sample][...]\n",
    "                        peaks = peaks - offset\n",
    "                        \n",
    "                        indices.append(peaks)\n",
    "                        dataset_ids.append(sample)\n",
    "                        \n",
    "                except OSError:\n",
    "                    print('\\n\\tError saving data for sample {}, factor: {}'\\\n",
    "                          .format(str(sample), sample_metadata.factor))\n",
    "                except KeyError:\n",
    "                    print('\\n\\tError: No metadata for sample {}'.format(str(sample)))\n",
    "                    \n",
    "        tf_matrix = indices_list_to_sparse_array(indices, num_bins)\n",
    "        \n",
    "        return tf_matrix, dataset_ids\n",
    "\n",
    "def write_tf_matrix(data, technology, dataset_ids, tf_matrix, metadata):\n",
    "    \n",
    "    tf_matrix = tf_matrix.T.tocsr()\n",
    "    \n",
    "    for i, dataset_id in enumerate(dataset_ids):\n",
    "        dataset_id = str(dataset_id)\n",
    "        save_indices = tf_matrix[i, :].indices\n",
    "        sample_metadata = metadata.loc[dataset_id, data.get_metadata_headers(technology)].to_dict()\n",
    "        data.add_binding_data(technology, dataset_id, save_indices, **sample_metadata)\n",
    "        \n",
    "def convert_tf_binding(*, data, technology, config_key, old_data_path, original_num_bins, hits_binmap,\n",
    "                metadata, offset = 0, skip_qc = False):\n",
    "    \n",
    "    tf_matrix, dataset_ids = load_tf_matrix(config_key, old_data_path, original_num_bins, \n",
    "                                        metadata, offset = offset, skip_qc = skip_qc)\n",
    "    \n",
    "    projected_tf_matrix = data.project_sparse_matrix(tf_matrix.T, hits_binmap, len(data.genome))\n",
    "    \n",
    "    write_tf_matrix(data, technology, dataset_ids, projected_tf_matrix, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChIP-seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tf_binding(data = data, technology = 'ChIP-seq', config_key = 'tf_chipseq', old_data_path = old_data_path, \n",
    "                  original_num_bins = len(old_genome_100bp), metadata = metadata, offset = 1, skip_qc = False,\n",
    "                  hits_binmap = hits_binmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motif data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tf_binding(data = data, technology = 'Motifs', config_key = 'genome_100bp_motif_index99', old_data_path = old_data_path,\n",
    "                original_num_bins = len(old_genome_100bp), metadata = motif_meta, offset = 0, skip_qc = True,\n",
    "                hits_binmap = hits_binmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100bp hits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_hitbin_data = data_interface.DataInterface(species, window_size=100, make_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_fine_binmap = np.array(old_genome_100bp.map_genomes(fine_hitbin_data.genome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tf_binding(data = fine_hitbin_data, technology = 'ChIP-seq', config_key = 'tf_chipseq', old_data_path = old_data_path, \n",
    "                  original_num_bins = len(old_genome_100bp), metadata = metadata, offset = 1, skip_qc = False,\n",
    "                  hits_binmap = hits_fine_binmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tf_binding(data = fine_hitbin_data, technology = 'Motifs', config_key = 'genome_100bp_motif_index99', old_data_path = old_data_path,\n",
    "                original_num_bins = len(old_genome_100bp), metadata = motif_meta, offset = 0, skip_qc = True,\n",
    "                hits_binmap = hits_fine_binmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc_matrix, dataset_ids = load_accessibility_data('DNase', old_data_path)\n",
    "\n",
    "projected_acc_matrix = np.vstack([\n",
    "    data.project_array(r, genome_map, len(data.genome))\n",
    "    for r in acc_matrix.T\n",
    "]).T\n",
    "\n",
    "add_accessibility_matrix('DNase', projected_acc_matrix, dataset_ids, \n",
    "                         rp_maps=[basic_rp_map, enhanced_rp_map], rp_map_styles=['basic_10K','enhanced_10K'],\n",
    "                        metadata = metadata, reads_range = (75000, 160000))\n",
    "\n",
    "acc_matrix, dataset_ids = load_accessibility_data('H3K27ac', old_data_path)\n",
    "\n",
    "projected_acc_matrix = np.vstack([\n",
    "    data.project_array(r, genome_map, len(data.genome))\n",
    "    for r in acc_matrix.T\n",
    "]).T\n",
    "\n",
    "projected_acc_matrix.shape\n",
    "\n",
    "add_accessibility_matrix('H3K27ac', projected_acc_matrix, dataset_ids, \n",
    "                         rp_maps=[basic_rp_map, enhanced_rp_map], rp_map_styles=['basic_10K','enhanced_10K'],\n",
    "                        metadata = metadata, reads_range = (130000, 160000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_matrix, dataset_ids = load_tf_matrix('genome_100bp_motif_index99', old_data_path, \n",
    "        len(old_genome_100bp), motif_meta, offset = 0, skip_qc = True)\n",
    "\n",
    "projected_tf_matrix = data.project_sparse_matrix(tf_matrix.T, hits_binmap, len(data.genome))\n",
    "\n",
    "write_tf_matrix('Motifs', dataset_ids, projected_tf_matrix, motif_meta)\n",
    "\n",
    "tf_matrix, dataset_ids = load_tf_matrix('tf_chipseq', old_data_path, len(old_genome_100bp), \n",
    "                                        metadata, offset = 1)\n",
    "\n",
    "projected_tf_matrix = data.project_sparse_matrix(tf_matrix.T, hits_binmap, len(data.genome))\n",
    "\n",
    "write_tf_matrix('ChIP-seq', dataset_ids, projected_tf_matrix, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lisa_2.1_testing",
   "language": "python",
   "name": "lisa_2.1_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
