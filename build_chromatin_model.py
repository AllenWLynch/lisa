
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import StandardScaler
import numpy as np

def build_chromatin_model(reg_potential_data, dataset_metadata, labels, sample_selection_model, chromatin_model, use_covariates = False, 
    num_anova_features = 200, n_jobs = 1):
    """
    reg_potential_data: numpy matrix of gene x sample of RP data
    labels: generated by background_genes_selection.py, lables for each gene in query vs. background 
    chromatin_model: GridSearchCV sklearn model that trains a model to fit the chromatin landscape. Changes in the activation of this model show strength of effect of ISD
    sample_selection_model: strategy to find most discriminatory datasets
    num_anova_features: number of features to select using ANOVA
    num_LR_features: number of features to select using the Logistic Regression L1 model
    """
    #assert( isinstance(reg_potential_data, np.ndarray ) ), 'Regulatory potential data must be provided as numpy array'
    #assert( isinstance(labels, (list, np.ndarray)) ), 'Labels provided must be of type numpy array'
    #assert( isinstance(dataset_metadata, (list, np.array)))
    assert( reg_potential_data.shape == (len(labels), len(dataset_metadata)) )

    #NORMALIZE THE DATA
    #take log2 of RP
    reg_potential_data = np.log2(reg_potential_data + 1)
    normalizer = StandardScaler(with_std = False)
    
    reg_potential_data = normalizer.fit_transform(reg_potential_data)
    
    dataset_metadata = np.array(dataset_metadata)

    #select features with anova
    anova_featues = SelectKBest(f_classif, k=num_anova_features).fit(reg_potential_data, labels).get_support()

    #leave out worst features
    reg_potential_data = reg_potential_data[:, anova_featues]
    dataset_metadata = dataset_metadata[anova_featues]
    normalization_means = normalizer.mean_[anova_featues]

    #select best features using LR, record some info about model
    sample_selection_model = sample_selection_model.fit(reg_potential_data, labels)
    selected_features = sample_selection_model.get_selected_datasets()

    #subset for best features
    reg_potential_data = reg_potential_data[:, selected_features] 
    dataset_metadata = dataset_metadata[selected_features]
    normalization_means = normalization_means[selected_features]

    #add covariates back to data
    if use_covariates and False:
        reg_potential_data = np.concatenate([reg_potential_data, covariates], axis = 1)

    #train chromatin landscape model
    chromatin_model.fit(reg_potential_data, labels, n_jobs)

    #return final trained model, the selected features' metadata (id), the sample selection model, chromatin model, and a normalization function
    return reg_potential_data, dataset_metadata, sample_selection_model, chromatin_model, lambda x : np.log2(x + 1) - normalization_means.reshape((1,-1))